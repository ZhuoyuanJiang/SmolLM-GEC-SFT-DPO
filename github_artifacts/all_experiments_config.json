{
  "sft_padding_bs128_lr5e-05_ep1": {
    "training_type": "sft",
    "method": "padding",
    "batch_size": 32,
    "learning_rate": 5e-05,
    "gradient_accumulation_steps": 4,
    "num_epochs": 1,
    "effective_batch_size": 128,
    "model_name": "HuggingFaceTB/SmolLM-135M",
    "optimizer": "adamw_torch",
    "weight_decay": 0.01,
    "warmup_ratio": 0.1,
    "max_seq_length": 512,
    "seed": 42,
    "experiment_name": "sft_padding_bs128_lr5e-05_ep1",
    "bleu_score": null,
    "final_loss": null,
    "eval_loss": null,
    "training_completed": true,
    "has_final_model": true
  },
  "sft_padding_bs128_lr8e-05_ep1": {
    "training_type": "sft",
    "method": "padding",
    "batch_size": 32,
    "learning_rate": 8e-05,
    "gradient_accumulation_steps": 4,
    "num_epochs": 1,
    "effective_batch_size": 128,
    "model_name": "HuggingFaceTB/SmolLM-135M",
    "optimizer": "adamw_torch",
    "weight_decay": 0.01,
    "warmup_ratio": 0.1,
    "max_seq_length": 512,
    "seed": 42,
    "experiment_name": "sft_padding_bs128_lr8e-05_ep1",
    "bleu_score": null,
    "final_loss": null,
    "eval_loss": null,
    "training_completed": true,
    "has_final_model": true
  },
  "sft_padding_bs16_lr5e-05_ep1": {
    "training_type": "sft",
    "method": "padding",
    "batch_size": 16,
    "learning_rate": 5e-05,
    "gradient_accumulation_steps": 1,
    "num_epochs": 1,
    "model_name": "HuggingFaceTB/SmolLM-135M",
    "optimizer": "adamw_torch",
    "weight_decay": 0.01,
    "warmup_ratio": 0.1,
    "max_seq_length": 512,
    "seed": 42,
    "experiment_name": "sft_padding_bs16_lr5e-05_ep1",
    "effective_batch_size": 16,
    "bleu_score": null,
    "final_loss": null,
    "eval_loss": null,
    "training_completed": true,
    "has_final_model": true
  },
  "sft_padding_bs16_lr8e-05_ep1": {
    "training_type": "sft",
    "method": "padding",
    "batch_size": 16,
    "learning_rate": 8e-05,
    "gradient_accumulation_steps": 1,
    "num_epochs": 1,
    "model_name": "HuggingFaceTB/SmolLM-135M",
    "optimizer": "adamw_torch",
    "weight_decay": 0.01,
    "warmup_ratio": 0.1,
    "max_seq_length": 512,
    "seed": 42,
    "experiment_name": "sft_padding_bs16_lr8e-05_ep1",
    "effective_batch_size": 16,
    "bleu_score": null,
    "final_loss": null,
    "eval_loss": null,
    "training_completed": true,
    "has_final_model": true
  },
  "sft_padding_bs32_lr5e-05_ep1": {
    "training_type": "sft",
    "method": "padding",
    "batch_size": 32,
    "learning_rate": 5e-05,
    "gradient_accumulation_steps": 1,
    "num_epochs": 1,
    "model_name": "HuggingFaceTB/SmolLM-135M",
    "optimizer": "adamw_torch",
    "weight_decay": 0.01,
    "warmup_ratio": 0.1,
    "max_seq_length": 512,
    "seed": 42,
    "experiment_name": "sft_padding_bs32_lr5e-05_ep1",
    "effective_batch_size": 32,
    "bleu_score": null,
    "final_loss": null,
    "eval_loss": null,
    "training_completed": true,
    "has_final_model": true
  },
  "sft_padding_bs32_lr8e-05_ep1": {
    "training_type": "sft",
    "method": "padding",
    "batch_size": 32,
    "learning_rate": 8e-05,
    "gradient_accumulation_steps": 1,
    "num_epochs": 1,
    "model_name": "HuggingFaceTB/SmolLM-135M",
    "optimizer": "adamw_torch",
    "weight_decay": 0.01,
    "warmup_ratio": 0.1,
    "max_seq_length": 512,
    "seed": 42,
    "experiment_name": "sft_padding_bs32_lr8e-05_ep1",
    "effective_batch_size": 32,
    "bleu_score": null,
    "final_loss": null,
    "eval_loss": null,
    "training_completed": true,
    "has_final_model": true
  },
  "sft_padding_bs64_lr5e-05_ep1": {
    "training_type": "sft",
    "method": "padding",
    "batch_size": 32,
    "learning_rate": 5e-05,
    "gradient_accumulation_steps": 2,
    "num_epochs": 1,
    "effective_batch_size": 64,
    "model_name": "HuggingFaceTB/SmolLM-135M",
    "optimizer": "adamw_torch",
    "weight_decay": 0.01,
    "warmup_ratio": 0.1,
    "max_seq_length": 512,
    "seed": 42,
    "experiment_name": "sft_padding_bs64_lr5e-05_ep1",
    "bleu_score": null,
    "final_loss": null,
    "eval_loss": null,
    "training_completed": true,
    "has_final_model": true
  },
  "sft_padding_bs64_lr8e-05_ep1": {
    "training_type": "sft",
    "method": "padding",
    "batch_size": 32,
    "learning_rate": 8e-05,
    "gradient_accumulation_steps": 2,
    "num_epochs": 1,
    "effective_batch_size": 64,
    "model_name": "HuggingFaceTB/SmolLM-135M",
    "optimizer": "adamw_torch",
    "weight_decay": 0.01,
    "warmup_ratio": 0.1,
    "max_seq_length": 512,
    "seed": 42,
    "experiment_name": "sft_padding_bs64_lr8e-05_ep1",
    "bleu_score": null,
    "final_loss": null,
    "eval_loss": null,
    "training_completed": true,
    "has_final_model": true
  },
  "sft_padding_bs8_lr5e-05_ep1": {
    "training_type": "sft",
    "method": "padding",
    "batch_size": 8,
    "learning_rate": 5e-05,
    "gradient_accumulation_steps": 1,
    "num_epochs": 1,
    "model_name": "HuggingFaceTB/SmolLM-135M",
    "optimizer": "adamw_torch",
    "weight_decay": 0.01,
    "warmup_ratio": 0.1,
    "max_seq_length": 512,
    "seed": 42,
    "experiment_name": "sft_padding_bs8_lr5e-05_ep1",
    "effective_batch_size": 8,
    "bleu_score": null,
    "final_loss": null,
    "eval_loss": null,
    "training_completed": true,
    "has_final_model": true
  },
  "sft_padding_bs8_lr8e-05_ep1": {
    "training_type": "sft",
    "method": "padding",
    "batch_size": 8,
    "learning_rate": 8e-05,
    "gradient_accumulation_steps": 1,
    "num_epochs": 1,
    "model_name": "HuggingFaceTB/SmolLM-135M",
    "optimizer": "adamw_torch",
    "weight_decay": 0.01,
    "warmup_ratio": 0.1,
    "max_seq_length": 512,
    "seed": 42,
    "experiment_name": "sft_padding_bs8_lr8e-05_ep1",
    "effective_batch_size": 8,
    "bleu_score": null,
    "final_loss": null,
    "eval_loss": null,
    "training_completed": true,
    "has_final_model": true
  },
  "sft_packing_bs16_lr3e-05_ep1": {
    "training_type": "sft",
    "method": "packing",
    "batch_size": 16,
    "learning_rate": 3e-05,
    "gradient_accumulation_steps": 1,
    "num_epochs": 1,
    "model_name": "HuggingFaceTB/SmolLM-135M",
    "optimizer": "adamw_torch",
    "weight_decay": 0.01,
    "warmup_ratio": 0.1,
    "max_seq_length": 512,
    "seed": 42,
    "experiment_name": "sft_packing_bs16_lr3e-05_ep1",
    "effective_batch_size": 16,
    "bleu_score": null,
    "final_loss": null,
    "eval_loss": null,
    "training_completed": true,
    "has_final_model": true
  },
  "sft_packing_bs16_lr5e-05_ep1": {
    "training_type": "sft",
    "method": "packing",
    "batch_size": 16,
    "learning_rate": 5e-05,
    "gradient_accumulation_steps": 1,
    "num_epochs": 1,
    "model_name": "HuggingFaceTB/SmolLM-135M",
    "optimizer": "adamw_torch",
    "weight_decay": 0.01,
    "warmup_ratio": 0.1,
    "max_seq_length": 512,
    "seed": 42,
    "experiment_name": "sft_packing_bs16_lr5e-05_ep1",
    "effective_batch_size": 16,
    "bleu_score": null,
    "final_loss": null,
    "eval_loss": null,
    "training_completed": true,
    "has_final_model": true
  },
  "sft_packing_bs4_lr3e-05_ep1": {
    "training_type": "sft",
    "method": "packing",
    "batch_size": 4,
    "learning_rate": 3e-05,
    "gradient_accumulation_steps": 1,
    "num_epochs": 1,
    "model_name": "HuggingFaceTB/SmolLM-135M",
    "optimizer": "adamw_torch",
    "weight_decay": 0.01,
    "warmup_ratio": 0.1,
    "max_seq_length": 512,
    "seed": 42,
    "experiment_name": "sft_packing_bs4_lr3e-05_ep1",
    "effective_batch_size": 4,
    "bleu_score": null,
    "final_loss": null,
    "eval_loss": null,
    "training_completed": true,
    "has_final_model": true
  },
  "sft_packing_bs4_lr5e-05_ep1": {
    "training_type": "sft",
    "method": "packing",
    "batch_size": 4,
    "learning_rate": 5e-05,
    "gradient_accumulation_steps": 1,
    "num_epochs": 1,
    "model_name": "HuggingFaceTB/SmolLM-135M",
    "optimizer": "adamw_torch",
    "weight_decay": 0.01,
    "warmup_ratio": 0.1,
    "max_seq_length": 512,
    "seed": 42,
    "experiment_name": "sft_packing_bs4_lr5e-05_ep1",
    "effective_batch_size": 4,
    "bleu_score": null,
    "final_loss": null,
    "eval_loss": null,
    "training_completed": true,
    "has_final_model": true
  },
  "sft_packing_bs8_lr3e-05_ep1": {
    "training_type": "sft",
    "method": "packing",
    "batch_size": 8,
    "learning_rate": 3e-05,
    "gradient_accumulation_steps": 1,
    "num_epochs": 1,
    "model_name": "HuggingFaceTB/SmolLM-135M",
    "optimizer": "adamw_torch",
    "weight_decay": 0.01,
    "warmup_ratio": 0.1,
    "max_seq_length": 512,
    "seed": 42,
    "experiment_name": "sft_packing_bs8_lr3e-05_ep1",
    "effective_batch_size": 8,
    "bleu_score": null,
    "final_loss": null,
    "eval_loss": null,
    "training_completed": true,
    "has_final_model": true
  },
  "sft_packing_bs8_lr5e-05_ep1": {
    "training_type": "sft",
    "method": "packing",
    "batch_size": 8,
    "learning_rate": 5e-05,
    "gradient_accumulation_steps": 1,
    "num_epochs": 1,
    "model_name": "HuggingFaceTB/SmolLM-135M",
    "optimizer": "adamw_torch",
    "weight_decay": 0.01,
    "warmup_ratio": 0.1,
    "max_seq_length": 512,
    "seed": 42,
    "experiment_name": "sft_packing_bs8_lr5e-05_ep1",
    "effective_batch_size": 8,
    "bleu_score": null,
    "final_loss": null,
    "eval_loss": null,
    "training_completed": true,
    "has_final_model": true
  },
  "dpo_final_model_lr1e-06_ep1": {
    "training_type": "dpo",
    "method": "dpo",
    "batch_size": 16,
    "learning_rate": 1e-06,
    "gradient_accumulation_steps": 1,
    "num_epochs": 1,
    "beta": 0.1,
    "model_name": "HuggingFaceTB/SmolLM-135M",
    "optimizer": "adamw_torch",
    "weight_decay": 0.01,
    "warmup_ratio": 0.1,
    "max_seq_length": 512,
    "seed": 42,
    "experiment_name": "dpo_final_model_lr1e-06_ep1",
    "effective_batch_size": 16,
    "bleu_score": null,
    "final_loss": null,
    "eval_loss": null,
    "training_completed": true,
    "has_final_model": true
  },
  "dpo_final_model_lr1e-07_ep1": {
    "training_type": "dpo",
    "method": "dpo",
    "batch_size": 16,
    "learning_rate": 1e-07,
    "gradient_accumulation_steps": 1,
    "num_epochs": 1,
    "beta": 0.1,
    "model_name": "HuggingFaceTB/SmolLM-135M",
    "optimizer": "adamw_torch",
    "weight_decay": 0.01,
    "warmup_ratio": 0.1,
    "max_seq_length": 512,
    "seed": 42,
    "experiment_name": "dpo_final_model_lr1e-07_ep1",
    "effective_batch_size": 16,
    "bleu_score": null,
    "final_loss": null,
    "eval_loss": null,
    "training_completed": true,
    "has_final_model": true
  },
  "dpo_final_model_lr3e-07_ep1": {
    "training_type": "dpo",
    "method": "dpo",
    "batch_size": 16,
    "learning_rate": 3e-07,
    "gradient_accumulation_steps": 1,
    "num_epochs": 1,
    "beta": 0.1,
    "model_name": "HuggingFaceTB/SmolLM-135M",
    "optimizer": "adamw_torch",
    "weight_decay": 0.01,
    "warmup_ratio": 0.1,
    "max_seq_length": 512,
    "seed": 42,
    "experiment_name": "dpo_final_model_lr3e-07_ep1",
    "effective_batch_size": 16,
    "bleu_score": null,
    "final_loss": null,
    "eval_loss": null,
    "training_completed": true,
    "has_final_model": true
  },
  "ipo_final_model_lr1e-06_ep1": {
    "training_type": "ipo",
    "method": "ipo",
    "batch_size": 16,
    "learning_rate": 1e-06,
    "gradient_accumulation_steps": 1,
    "num_epochs": 1,
    "beta": 0.1,
    "model_name": "HuggingFaceTB/SmolLM-135M",
    "optimizer": "adamw_torch",
    "weight_decay": 0.01,
    "warmup_ratio": 0.1,
    "max_seq_length": 512,
    "seed": 42,
    "experiment_name": "ipo_final_model_lr1e-06_ep1",
    "effective_batch_size": 16,
    "bleu_score": null,
    "final_loss": null,
    "eval_loss": null,
    "training_completed": true,
    "has_final_model": true
  },
  "ipo_final_model_lr1e-07_ep1": {
    "training_type": "ipo",
    "method": "ipo",
    "batch_size": 16,
    "learning_rate": 1e-07,
    "gradient_accumulation_steps": 1,
    "num_epochs": 1,
    "beta": 0.1,
    "model_name": "HuggingFaceTB/SmolLM-135M",
    "optimizer": "adamw_torch",
    "weight_decay": 0.01,
    "warmup_ratio": 0.1,
    "max_seq_length": 512,
    "seed": 42,
    "experiment_name": "ipo_final_model_lr1e-07_ep1",
    "effective_batch_size": 16,
    "bleu_score": null,
    "final_loss": null,
    "eval_loss": null,
    "training_completed": true,
    "has_final_model": true
  },
  "ipo_final_model_lr3e-07_ep1": {
    "training_type": "ipo",
    "method": "ipo",
    "batch_size": 16,
    "learning_rate": 3e-07,
    "gradient_accumulation_steps": 1,
    "num_epochs": 1,
    "beta": 0.1,
    "model_name": "HuggingFaceTB/SmolLM-135M",
    "optimizer": "adamw_torch",
    "weight_decay": 0.01,
    "warmup_ratio": 0.1,
    "max_seq_length": 512,
    "seed": 42,
    "experiment_name": "ipo_final_model_lr3e-07_ep1",
    "effective_batch_size": 16,
    "bleu_score": null,
    "final_loss": null,
    "eval_loss": null,
    "training_completed": true,
    "has_final_model": true
  }
}